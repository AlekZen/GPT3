{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings con OpenAI\n",
    "Embeddings es un proceso mediante el cual se utiliza alguna tecnica/algoritmo que sea capaz de convertir palabras o texto a vectores de N dimensiones. Estos vectores contienen cierto nivel de información semantica sobre el texto o palabra. Por ejemplo, palabras que son muy similares van a tener valores cercanos en sus representaciones en vectores.\n",
    "\n",
    "Hay varios modelos que son capaces de hacer un embedding de nuestro texto, en este cuaderno estaremos utilizando el embedding de OpenAI, el cual tiene la capacidad de posicionas muy bien palabras o textos segun su semantica. Este es el mismo embedding que utiliza GPT3. En este cuaderno estarás haciendo embedding de un texto para despues poder buscar cosas dentro de este texto por medio de preguntas en lenguaje natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos todas las dependencias requeridas, en este caso será Gradio para desarrollar la interfaz grafica y openai para realizar los llamados a su API \n",
    "import gradio as gr\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "from openai.embeddings_utils import get_embedding\n",
    "from openai.embeddings_utils import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la API Key para vincular el cuaderno con nuestra cuenta de OpenAI\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Que es y cómo usar embeddings\n",
    "Al hacer embedding de un dato, lo estamos convirtiendo a un vector numérico, datos similares estarán más cercanos entre si cuando semanticamente son similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se puede hacer embeeding de palabras o cadenas de texto\n",
    "palabras = [\"casa\", \"perro\", \"gato\", \"lobo\", \"leon\", \"zebra\", \"tigre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario = {}\n",
    "for i in palabras:\n",
    "    diccionario[i] = get_embedding(i, engine=\"text-embedding-ada-002\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['casa', 'perro', 'gato', 'lobo', 'leon', 'zebra', 'tigre'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diccionario.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 10 valores de gato:\n",
      " [-0.024532638490200043, 0.009612735360860825, -0.0003203161177225411, -0.011186674237251282, -0.00718028424307704, 0.011908605694770813, -0.021970108151435852, -0.024909863248467445, -0.005206356290727854, -0.0190043393522501]\n",
      "\n",
      "\n",
      "Número de dimensiones del dato embebido\n",
      " 1536\n"
     ]
    }
   ],
   "source": [
    "palabra = \"gato\"\n",
    "print(\"Primeros 10 valores de {}:\\n\".format(palabra), diccionario[palabra][:10])\n",
    "print(\"\\n\")\n",
    "print(\"Número de dimensiones del dato embebido\\n\", len(diccionario[palabra]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar dos embeddings\n",
    "Debido a que los embeddings son una representacion vectorial de los datos en un espacio latente, podemos medir la distancia entre dos vectores y asi obtener que tan similares son. Podemos comparar una palabra nueva o alguna de las que ya fueron embebidas \n",
    "OJO: No necesariamente es similitud al objeto. Ej. perro y gato aun siendo \"opuestos\" semanticamente estan cerca pues tienen una relación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9586340528629816\n"
     ]
    }
   ],
   "source": [
    "n_palabra = \"perrito\" # Palabra nueva a comparar\n",
    "palabra_comparar = \"perro\" # Palabra del diccionario con la que compararemos la nueva palabra\n",
    "n_palabra_embed = get_embedding(n_palabra, engine=\"text-embedding-ada-002\")\n",
    "similitud = cosine_similarity(diccionario[palabra_comparar], n_palabra_embed)\n",
    "print(similitud)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumar embeddings\n",
    "Como los vectores contienen valores numericos, podemos sumarlos y el resultado será un nuevo vector de un concepto que una los elementos sumados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casa : [0.82819415]\n",
      "perro : [0.85046401]\n",
      "gato : [0.8563925]\n",
      "lobo : [0.85639036]\n",
      "leon : [0.9531031]\n",
      "zebra : [0.95310309]\n",
      "tigre : [0.88060081]\n"
     ]
    }
   ],
   "source": [
    "# Suma dos listas usando pandas\n",
    "sumados = (pd.DataFrame(diccionario[\"leon\"])) + (pd.DataFrame(diccionario[\"zebra\"]))\n",
    "len(sumados)\n",
    "\n",
    "for key, value in diccionario.items():\n",
    "    print(key, \":\", cosine_similarity(diccionario[key], sumados))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicacion de un Chatbot\n",
    "\n",
    "Usaremos Gradio para hacer una interfaz básica donde podremos hacer preguntas y obtendremos una respuesta. \n",
    "Para esto reutilizaremos lo que hemos visto hasta el momento pero usaremos el archivo de **chatbot_qa.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embed_text(path=\"texto.csv\"):\n",
    "    conocimiento_df = pd.read_csv(path)\n",
    "    conocimiento_df['Embedding'] = conocimiento_df['texto'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
    "    conocimiento_df.to_csv('embeddings.csv')\n",
    "    return conocimiento_df\n",
    "\n",
    "def buscar(busqueda, datos, n_resultados=2):\n",
    "    busqueda_embed = get_embedding(busqueda, engine=\"text-embedding-ada-002\")\n",
    "    datos[\"Similitud\"] = datos['Embedding'].apply(lambda x: cosine_similarity(x, busqueda_embed))\n",
    "    datos = datos.sort_values(\"Similitud\", ascending=False)\n",
    "    return datos.iloc[:n_resultados][[\"texto\", \"Similitud\", \"Embedding\"]]\n",
    "\n",
    "texto_emb = embed_text(\"./chatbot_qa.csv\")\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    busqueda = gr.Textbox(label=\"Buscar\")\n",
    "    output = gr.DataFrame(headers=['texto'])\n",
    "    greet_btn = gr.Button(\"Preguntar\")\n",
    "    greet_btn.click(fn=buscar, inputs=[busqueda, gr.DataFrame(texto_emb)], outputs=output)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesar datos de un PDF\n",
    "Haremos ahora un ejemplo donde leemos un PDF para poder hacer preguntas y traer un exctracto del PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (0.0.144)\n",
      "Requirement already satisfied: pypdf in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (3.8.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (5.4.1)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (1.4.18)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (1.24.2)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (1.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (2.28.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (8.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from pypdf) (4.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from requests<3,>=2->langchain) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from requests<3,>=2->langchain) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from requests<3,>=2->langchain) (2021.10.8)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from SQLAlchemy<2,>=1->langchain) (1.1.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (20.9)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\alek\\appdata\\roaming\\python\\python39\\site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain pypdf\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"./mtg.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jasper SandnerC 214/ 269 2/1Oo2\\n™ & © 2014 Wizards of the CoastSP• M15Campo de \\nbatallatú \\n16 vidas\\nrestantes\\nyo\\n18 vidas \\nrestantesCementerio  Biblioteca  \\nBiblioteca  Mano\\nManoCementerio  \\n3Para comenzar el juego, baraja tu mazo, \\ntambién conocido como tu biblioteca. Roba una mano de siete cartas y comprueba cuántas tierras tienes. Puedes mirar la línea de texto que hay bajo la ilustración de cada carta para ver de qué tipo de carta se trata. Para este primer juego, si no tienes al menos dos tierras, baraja de nuevo tu mazo (incluyendo tu mano anterior) y roba una mano nueva.\\nCada jugador comienza con 20 vidas, \\ny cada uno debe llevar la cuenta de su total de vidas de alguna manera (con un dado, lápiz y papel...). ¡Reduce el total de vidas de tu oponente a 0 y ganarás el juego!Comenzar'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un elemento por cada página\n",
    "pages[3].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objeto que va a hacer los cortes en el texto\n",
    "split = CharacterTextSplitter(chunk_size=300, separator = '.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 364, which is longer than the specified 300\n",
      "Created a chunk of size 326, which is longer than the specified 300\n",
      "Created a chunk of size 325, which is longer than the specified 300\n",
      "Created a chunk of size 503, which is longer than the specified 300\n",
      "Created a chunk of size 1507, which is longer than the specified 300\n",
      "Created a chunk of size 308, which is longer than the specified 300\n",
      "Created a chunk of size 583, which is longer than the specified 300\n",
      "Created a chunk of size 458, which is longer than the specified 300\n",
      "Created a chunk of size 429, which is longer than the specified 300\n",
      "Created a chunk of size 626, which is longer than the specified 300\n",
      "Created a chunk of size 358, which is longer than the specified 300\n",
      "Created a chunk of size 892, which is longer than the specified 300\n",
      "Created a chunk of size 1311, which is longer than the specified 300\n",
      "Created a chunk of size 333, which is longer than the specified 300\n",
      "Created a chunk of size 1139, which is longer than the specified 300\n",
      "Created a chunk of size 568, which is longer than the specified 300\n",
      "Created a chunk of size 916, which is longer than the specified 300\n",
      "Created a chunk of size 619, which is longer than the specified 300\n",
      "Created a chunk of size 910, which is longer than the specified 300\n",
      "Created a chunk of size 742, which is longer than the specified 300\n",
      "Created a chunk of size 1074, which is longer than the specified 300\n",
      "Created a chunk of size 706, which is longer than the specified 300\n",
      "Created a chunk of size 538, which is longer than the specified 300\n",
      "Created a chunk of size 427, which is longer than the specified 300\n",
      "Created a chunk of size 324, which is longer than the specified 300\n",
      "Created a chunk of size 867, which is longer than the specified 300\n",
      "Created a chunk of size 644, which is longer than the specified 300\n",
      "Created a chunk of size 447, which is longer than the specified 300\n",
      "Created a chunk of size 304, which is longer than the specified 300\n",
      "Created a chunk of size 440, which is longer than the specified 300\n",
      "Created a chunk of size 315, which is longer than the specified 300\n",
      "Created a chunk of size 376, which is longer than the specified 300\n",
      "Created a chunk of size 506, which is longer than the specified 300\n",
      "Created a chunk of size 387, which is longer than the specified 300\n",
      "Created a chunk of size 309, which is longer than the specified 300\n",
      "Created a chunk of size 319, which is longer than the specified 300\n",
      "Created a chunk of size 414, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "textos = split.split_documents(pages) # Lista de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guía de inicio \n",
      "rápidoEdad: 13 o +\n"
     ]
    }
   ],
   "source": [
    "print(textos[0].page_content)\n",
    "#print(textos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 texto\n",
      "0                  Guía de inicio \\nrápidoEdad: 13 o +\n",
      "1    2Bienvenido a Magic: The Gathering, el mejor j...\n",
      "2    Si ya tienes algunas cartas de Magic, es el mo...\n",
      "3    Hay miles de cartas para elegir, por lo \\nque ...\n",
      "4    Siempre que ganes vidas, puedes \\nponer un con...\n",
      "..                                                 ...\n",
      "107  ∙Paso de mantenimiento\\n ∙Paso de robar: roba ...\n",
      "108  Fase de combate\\n ∙Paso de inicio del combate\\...\n",
      "109  ∙ Paso de declarar bloqueadoras: cada criatura...\n",
      "110  ∙ Paso de daño de combate: las criaturas bloqu...\n",
      "111  ∙Paso de final del combate.\\nFase principal (d...\n",
      "\n",
      "[112 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extraemos la parte de page_content de cada texto y lo pasamos a un dataframe\n",
    "textos = [str(i.page_content) for i in textos] #Lista de parrafos\n",
    "parrafos = pd.DataFrame(textos, columns=[\"texto\"])\n",
    "print(parrafos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "parrafos['Embedding'] = parrafos[\"texto\"].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002')) # Nueva columna con los embeddings de los parrafos\n",
    "parrafos.to_csv('MTG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La misma funcion del chatbot de pregunts y respuestas\n",
    "def embed_text(path=\"texto.csv\"):\n",
    "    conocimiento_df = pd.read_csv(path)\n",
    "    conocimiento_df['Embedding'] = conocimiento_df['texto'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
    "    conocimiento_df.to_csv('mtg-embeddings.csv')\n",
    "    return conocimiento_df\n",
    "\n",
    "def buscar(busqueda, datos, n_resultados=5):\n",
    "    busqueda_embed = get_embedding(busqueda, engine=\"text-embedding-ada-002\")\n",
    "    datos[\"Similitud\"] = datos['Embedding'].apply(lambda x: cosine_similarity(x, busqueda_embed))\n",
    "    datos = datos.sort_values(\"Similitud\", ascending=False)\n",
    "    return datos.iloc[:n_resultados][[\"texto\", \"Similitud\", \"Embedding\"]]\n",
    "\n",
    "texto_emb = parrafos\n",
    "with gr.Blocks() as demo:\n",
    "    busqueda = gr.Textbox(label=\"Buscar\")\n",
    "    output = gr.DataFrame(headers=['texto'])\n",
    "    greet_btn = gr.Button(\"Preguntar\")\n",
    "    greet_btn.click(fn=buscar, inputs=[busqueda, gr.DataFrame(texto_emb)], outputs=output)\n",
    "\n",
    "demo.launch()\n",
    "\n",
    "\n",
    "# resp = buscar(\"Con cuanta vida empiezo?\", parrafos, 5) # Reutilizamos la funcion de \"buscar\" del app de gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp = buscar(\"Con cuanta vida empiezo?\", parrafos, 5) # Reutilizamos la funcion de \"buscar\" del app de gradio\n",
    "# print(resp.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('gpt3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3e99415c7ce2cb9b8857283077fa90c1d7ffec737ddc9f365b1225d7f13a404"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
